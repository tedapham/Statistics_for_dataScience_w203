---
title: "Lab 4: Does Prenatal Care Improve Infant Health?"
author: "w203: Statistics for Data Science"

date: "April 10, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The National Center for Health Statistics published a dataset (bwght\_w203.RData) about prenatal care and infant health. Our group has been hired by a health advocacy group to study the dataset and help them understand whether prenatal care improves health outcomes for newborn infants. 

The data file includes a birthweight variable. Additonally, the one- and five-minute APGAR scores are included. These are measures of the well being of infants just after birth.

Variable descriptions are provided as follows.

```{r results='hide', message=FALSE, warning=FALSE}
library(car)
library(leaps)
library(MASS)
library(Hmisc)
library(lmtest)
library(sandwich)
library(stargazer)
library(effsize)
```

```{r}
load("bwght_w203.RData")
desc
attach(data)
dim(data)
```

There are 23 variables and 1832 observations. The variables can be categorized into 4 groups: 

* Father's demographic characteristics (age, education, ethnicity)
* Mother's demographic characteristics (age, education, ethnicity)
* Prenatal care
* Smoking and drinking by the mother

In this report, we are interested in learning if prenatal care improves infant health. Birth weight and the two APGAR scores are common measures of health at birth. 

## Exploratory analysis and model Building

We try to build a multiple linear regression to study the relationship between prenatal care and infant health. Our goal is to find a model that contains fewer predictors but can explain much of the variance in the response variable. 

### Determine the response variable

Among the 23 variables, bwght, omaps, fmaps, and lbwght are four potential response variables. The bwght and lbwght variables measure the birth weight and log of birth weight of newborn infants. The omaps (1-minute score) determines how well the infant tolerated the birth process and fmaps (5-minute score) informs doctor how well the baby is doing after leaving monther's womb. Thus, we believe that **fmaps** is the most accurate measurement of infant health and will use it as the response variable.

### Remove irrevalent variables

To assess lbw (=1 if bwght <= 2000) and vlbw(=1 if bwght <= 1500) variables, let's take a look at the summary of bwght. 

```{r}
summary(bwght)
hist(bwght)
table(lbw)
table(vlbw)
```

We see the mean of birth weight (bwght) is 3402 grams and the 1st Quantile is 3076 grams. Both lbw and vlbw variables indicate very underweight infants. lbw has 30 observations and vlbw has only 13 observations. Their sample sizes are very small. There is no practical meaning to include them in our model. 

We also notice that father's demographic characteristics are not directly related to infant health. We decide to not consider father's demo in the model building process. 

### Create a new variable: `mrace`

Next, we look at mother's demographic characteristics. We decide to create a new variable of mother's race, called mrace, by recoding mwhte, mblck, and moth. 

mrace = 1 if mother white, mrace = 2 if mother black, and mrace = 3 if mother other race.

```{r}
data$mrace[data$mwhte == 1] = 1 # if mother white
data$mrace[data$mblck == 1] = 2 # if mother black
data$mrace[data$moth == 1] = 3  # if mother other race
summary(data$mrace)
table(data$mrace)
```

We see the vast majority of mother are white (1624/1832=88.64%). Only 109+99=208 (11.35%) mothers are non-white. This contradicts to the real-life population distribution given non-white groups have much higher birth rate. We wonder this is a sampling bias but we don't look into this issue in this report.

### Determine which variables to use

We start building our model based on the following variables: 

*Response: fmaps

*Predictors: mage, meduc, monpre, npvis, cigs, drink, male, mrace, magesq, npvissq

First, we take a look at the scatterplotMatrix to roughly determine if we have a linear correlation between multiple variables. We also compute the correlation matrix to investigate the dependence between multiple variables. 

```{r warning=FALSE}
scatterplotMatrix(data[, c("fmaps","mage","meduc","monpre","npvis","cigs","drink","male","mrace","magesq","npvissq")])

round(rcorr(as.matrix(data[, c("fmaps","mage","meduc","monpre","npvis","cigs","drink","male","mrace","magesq","npvissq")]))$r,4)
```

Based on the correlation matrix, monpre, cigs, and male have slightly negative correlation with fmaps. npvis and npvissq have significant positive correlation with fmaps. mage, meduc, drink, mrace, and magesq have slightly positive correlation with fmaps but the correlation coefficients are insignificant. 

Note that magesq and npvissq are mage^2 and npvis^2. As discussed in class before, the linear regression assumes a linear relationship between the response and predictors. But in some cases, the true relationship between the response and the predictor may be non-linear. In this case we have to extend the linear model to accommodate non-linear relationships, using polynomial regression. 

In fact, we do intend to include the **quadratic transformation** in our model because mother's age (mage) and total number of prenatal visits (npvis) have non-linear relationship with fmaps. Very young (<20) and very old mother (>35) are possible to have poor health infants, while middle aged mother are more likely to give birth to healty infants. Similarly, mothers who need prenatal care frequently are more likely to have poor health infants. So the relationship between fmaps and npvis is not perfectly linear.  

Consider the following scatter plots of npvis vs. fmaps and mage vs. fmaps. The number of npvis, mage, and fmaps are shown.

```{r warning=FALSE}
par(mfrow=c(1,2))
plot(npvis,fmaps)
abline(lm(fmaps~npvis, data=data), col="blue")
abline(lm(fmaps~npvis+npvissq, data=data), col="red")

plot(mage,fmaps)
abline(lm(fmaps~mage, data=data), col="blue")
abline(lm(fmaps~mage+magesq, data=data), col="red")
par(mfrow=c(1,1))
```

For each plot, the blue line represents the linear regression fit. Although it appears to have a linear relationship, we think the data suggest a curved relationship. The red curve contains the quadratic term and may provide a better fit. After all, it is still a linear model. 

### Build our models

We start this process by testing multiple models. Our three model specifications: `fit1`, `fit2` and `fit3` will be presented at the end of this section, and we will do a thorough review of the three models performance. 

First, We fit a model called `null1` with zero predictor and full model called `full1` containing all possible predictors. 

```{r}
# null model
null1 = lm(fmaps ~ 1, data = data)
summary(null1)

# full model
full1 = lm(fmaps ~mage+meduc+monpre+npvis+cigs+drink+male+mrace+magesq+npvissq, data=data)
summary(full1)
```

The model p-value: 0.001956 shows that the full model is statistical significant different than the null model. Besides, variables npvis and npvissq are statistically significant at 95%. The coefficients' p-values indicate that npvis and npvissq are related to fmaps. Note that the coefficient estimate of npvis is positive but the coefficient estimate of npvissq is negative. This means that if a mother seeks a large number of prenantal care, it's likely to have a poor health infant. The adjusted $R^2$ = 0.01089. The model explains 1.089% of the variance in the response varialbe fmaps.

Based on the full model, we use `setp()` function to perform **variable selection** by running backward elimination and stepwise regression.

```{r}
data1 <- na.omit(data[, c("fmaps","mage","meduc","monpre","npvis","cigs","drink","male","mrace","magesq","npvissq")])
full1.1 = lm(fmaps ~mage+meduc+monpre+npvis+cigs+drink+male+mrace+magesq+npvissq, data=data1)
null1.1 = lm(fmaps ~1, data=data1)
#step(null1.1, scope = list(upper=full1.1), data=data1, direction="both")
step(full1.1, data=data1, direction = "forward")
```

Both backward elimination and stepwise regression give us the same results. Variables npvis and npvissq should be included in our best model. 

Recall the correlation matrix, the correlation between fmaps and meduc is 0.03 and the correlation between fmaps and mrace is approximately 0.00. We believe that meduc and mrace have very slim impact on infant health. The scatter plots with abline are shown below.

We also suspect that cigarettes and drinks have very slim impact on infant health because the correlation between fmaps and cigs is -0.01 and the correlation between fmaps and drink is 0.02. The correlation coefficients are very small. 

In addition, the correlation between fmaps and male is -0.02. Male infants have slightly worse fmaps score. The correlation is insignificant at 95%. We perform a **two-sample independent t-test** to compare the mean fmaps value for male and femle infants.

Null hypothesis: mean fmaps value for male is equal to mean fmaps value for female

Alternative hypothesis: mean fmaps value for male is NOT equal to mean fmaps value for female

```{r}
# t-test comparision of male and female fmaps score. 
male.fmaps <- data$fmaps[data$male == 1]
female.fmaps <- data$fmaps[data$male == 0]
t.test(male.fmaps, female.fmaps)
```

The mean fmaps score for male infants is 8.995, and the mean fmaps score for female infants is 9.01. Since the 
p-value = 0.3997, we do not reject the null hypothesis and conclude that there is NO difference in mean fmaps between male infants and female infants.

```{r}
par(mfrow=c(2,3))
plot(cigs,fmaps)
abline(lm(fmaps~cigs, data = data), col="blue")

plot(drink,fmaps)
abline(lm(fmaps~drink, data = data), col="blue")

plot(meduc, fmaps)
abline(lm(fmaps~meduc, data=data), col="blue")

plot(data$mrace, fmaps)
abline(lm(fmaps~mrace, data=data), col="blue")

plot(male, fmaps)
abline(lm(fmaps~male, data=data), col="blue")

```

For each scatter plot, the regression abline appears to be a constant line. This means that cigs, drink, meduc, mrace, and male have slim or virtually no impact on fmaps. Thus we can exclude those five variables and fit our second model `full2` with mage, monpre, npvis, magesq and npvissq. 

```{r}
full2=lm(fmaps ~mage+monpre+npvis+magesq+npvissq, data=data)
summary(full2)
```

The p-value: 5.42e-06 shows that the `full2` model is statistically significant different than the null model. Variables npvis and npvissq are statistically significant at 95%. The coefficients' p-values indicate that both npvis and npvissq are related to fmaps. The adjusted $R^2$ = 0.01538. The model explains 1.538% of the variance in the response varialbe fmaps.

To determine whether to include both mage and magesq in the same model. We use `anova()` function to perform a hypothesis test comparing the two models.

Null hypothesis: the two models fit the data equally well.

Alternative hypothesis: the full model fits data better than the reduced model.

```{r}
m0 <- lm(fmaps~mage, data=data)
m1 <- lm(fmaps~mage+magesq, data=data)
anova(m0, m1)
```

The F-statistic is 0.1202 and the associated p-value is 0.7289 > 0.05. So we do not reject the null and conclude that the two models fit the data equally well and we don't have evidence to include both mage and magesq. We decide to include only mage in the model. 

Similarly, we compare the two models for npvis and npvissq.

```{r}
m2 <- lm(fmaps~npvis, data=data)
m3 <- lm(fmaps~npvis+npvissq, data=data)
anova(m2,m3)
```

The F-statistic is 11.672 and the associated p-value is 0.0006489 < 0.05. So we reject the null and conclude that the model containing both npvis and npvissq is far superior to the model that only contains npvis. 

Finally, we have four predictors (mage, monpre, npvis and npvissq) to construct the model with only the explanatory variables of key interest. 

## Three model specifications

### Model with only the explanatory variables of key interest

Our first model, `fit1`, contains predictors: mage, monpre, npvis and npvissq. They are the explanatory variables of key interest. 

```{r}
fit1 = lm(fmaps ~mage+monpre+npvis+npvissq, data=data)
summary(fit1)
```

The model p-value: 1.85e-06 shows that the `fit1` model is statistically significant different than the null model. Variables npvis and npvissq are statistically significant at 95%. The coefficients' p-values indicate that npvis and npvissq are related to fmaps. The coefficient estmate for npvis is 0.046, which means if there is one unit increase in npvis, fmaps value will increase by 0.046. Note that the coefficient estimate of npvis is positive but the coefficient estimate of npvissq is negative. This means that if a mother seeks a large number of prenantal care, it's likely to have a poor health infant. The adjusted $R^2$ = 0.01586 shows that `fit1` fits the data better than full1 and full2. The model explains 1.586% of the variance in the response varialbe fmaps.


### Model that includes only covariates that you believe increase the accuracy of the results without introducing bias.

Our second model, `fit2`, contains an interaction term mage$\times$npvis. We believe that npvis has a different effect on fmaps depending on the values of mage. 

```{r}
fit2=lm(fmaps ~mage+monpre+npvis+npvissq+mage*npvis, data=data)
summary(fit2)

#compare two models
anova(fit1, fit2)
```

Here, the Adjusted R-squared=0.01757, which has increased from 0.01586 in `fit1`. Based on the coefficients' p-value, variables mage, npvis, npvissq, and mage$\times$npvis are all significant at 95%. The `anova()` function compares two models `fit1` and `fit2` and produces p-value = 0.04406 < 0.05. This provides evidence that the model `fit2` is superior to `fit1`. Thus, adding the interaction term has increased the accuracy of our results without introducing bias. The model explains 1.757% of the variance in the response varialbe fmaps.   

### Model that includes the previous covariates, but also covariates that may be problematic for one reason or another

We find that variable drink may be problematic. Recall the correlation matrix, the correlation between fmaps and drink is 0.024, which is positive. However, we normally believe drink should have a negative relationship with infant health. The distribution of drink is actually very skewed. 1701 observations have zero drinks. Only 16 observations have positive drink values. The sample size of drink is too small so it's problematic if we include drink in our model. 

The model `fit3` contains a problematic variable drink. 
```{r}
table(drink)
hist(drink, breaks = 100)
fit3=lm(fmaps ~mage+monpre+npvis+npvissq+mage*npvis+drink, data=data)
summary(fit3)
```

The Adjusted R-squared= 0.01733, which is slight less than model `fit2`. 

## CLASSICAL LINEAR MODEL (CLM)
The following 6 assumptions will be explained based on our `fit1` model (`fit1=lm(fmaps ~mage+npvis+npvissq, data=data)`).

**NOTE: Explanations below are for fit1 and fit2. Fit3 represents a model where problem causing variables are induced so we did not waste time analyzing this model as we know it will violate these assumpitons further and rather explored our important models in better detail.

1. CLM 1 - Linearity
Though there is no test or rigorous proof that displays why the model is linear, we can look at the actual model and note that our explanatory variables themselves take the linear and polynomial form (squared value of number of visits), and the coefficients are left linear in nature. (Same applies for fit2).

2. CLM 2 - Random Sampling
It is important to note that no background was given as to how this data was sampled. We do not know if this is in fact a random sample that represents the true population sampled from. As we have described earlier in this report, examination of one of the variables, race, allows us to see the disparity in the number of white versus black and other races of the mothers. Is this an accurate representation of the population or is this the result of a sampling bias? We cannot determine this. However, to fit and analyze a linear model, we mus assume that random sample (but we are weary of it). (Same applies for fit2)

3. CLM 3 - Multi-Collinearity?
```{r}
vif(fit1) 

scatterplotMatrix(data[ , c("mage", "npvis", "npvissq")])
```

When we run vif on our fit1 model, we see here that the VIF (1.04, 1.25) for 'mage' and 'monpre' are relatively low so they do not cause too much concern. However, the VIF for both 'npvis' and 'npvissq' are 9.35 and 8.74, which are prett high VIF values. However, we note that 'npvis' and 'npvissq' are transformations of each other, allowing to expect some correlation. As explained in the introduction, we are aware and intend on including both the original and the transformed variable in our model. (We note the correlation from our scatterplot matrix as well). But none of our VIF are 10 or greater, so there are likely no LARGE standard errors that would affect our model.

```{r}
vif(fit2)
```
Looking at the VIF values for model for our fit2 causes a little more concern because of the high value for mage:npvis, but again this makes sense as it is a variable transformation from two of our variables already in the model, so correlation is expected. Once again, we refer to our introduction as to our explanation for keeping the variables.

4. CLM 4 - Zero Conditional Mean (CLM 4' - Exogeneity)
```{r}
plot(fit1, which = 1)
```

Looking at our Residual vs. Fitted values plot for our fit1 model, right off the bat we can see an interesting result. It seems like we do not have a collective group of data, but rather we have groupings (layers) of data points in a stepwise manner. We need to remember that our outcome variable, 'fmaps' is an ordinal variable, an integer value form 1-10, not a continous variable. Our plot reflects the ordinal nature of the variable. 

But looking at the model overall, it seems to relatively keep with the zero conditional mean assumption, wiht a slight skew upward near the left side of the plot. The slight skew could cause us to assume a less stringent version of the zero conditional mean: exogeneity, where our variables are uncorrelated with the error term.

However, let us for one second consider our model and think about the possibility of endogeneity within our variables for fit1 (mage, monpre, npvis, npvissq). Mother's age is likely to not have any other hidden factor that could effect our error, likewise for starting month of prenatal care. However with number of visits, we might be able to deduce another factor: wealth from this. If a mother can afford more visits, it is not unreasonable to assume that a baby's prenatal care in a wealthy family differs in treatment than that of a low income family. This is a bit of a stretch and an extrapolation that we cannot necessarily deduce from the information given, but we wanted to explore an explanation for the possibility of endogeneity.

Regardless, it is convenient that we can rely on assymptotics because of the large sample size of our data. Though a couple of the other assumptions of our 6 CLM statements are a little wishy washy, this statement is an important one because endogeneity (violation of zero conditional mean) would imply that our OLS coefficients are biased and inconsistent. 

5. CLM 5 - Homoskedasticity 
```{r}
plot(fit1, which = 3)
plot(fit1, which = 1)

```

To assess the if our data is in keeping with homoskedasticity, we look at a scale-location plot. The unlinear shape of in data points shows us that the errors are very likely heteroskedastic.

We can also assess homoskedasticity from our Residual vs. Fitted plot, looking at band thickness across the x's and we can very clearly see that the variance in the error is in fact not constant. So we are working with a heteroskedastic model.

To account for the heterskedasticity, we will use be using the White standard errors which are robust to heterskedasticity.
```{r}
coeftest(fit1, vcov = vcovHC)
```

Looking at the Residual vs. Fitted and Scale Location plot for our fit2 (our better model), we see barely any change (not shown because change in graphs from fit1 to fit2 are minimal), indicating heterdoskedasticity for even our best model, so we shall use White standard errors for that model as well.

6. CLM 6 - Normal Distribution of Errors

```{r}
hist(fit1$residuals, main = "Fit 1 Residuals", xlab = "Errors")

#Q-Q Plot
plot(fit1, which = 2)
```

From just our histogram of the residuals of our model, we can tell that our data is somewhat normally distributed, but there is a slight deviation from normality. There is a slight skew to to the left. 

We then look at our Q-Q plot, we can see the plot reflect the ordinal nature of our outcome variable, so we will take that into account. Otherwise, the plot is somewhat diagnol in nature. In any case, we can rely on asympototics of the our large sample set of data to reduce the effect of any error from our model.


\newpage
##Regression Table
5. A well-formatted regression table summarizing your model results.  Make sure that standard errors presented in this table are valid.  Also be sure to comment on both statistical and practical significance.

We take the vectors of robust standard errors 
```{r}
se.fit1 = sqrt(diag(vcovHC(fit1)))
se.fit2 = sqrt(diag(vcovHC(fit2)))
se.fit3 = sqrt(diag(vcovHC(fit3)))
```

We use the robust standard errors in the summary of the model results
```{r}
stargazer(fit1, fit2, fit3, 
          title = "Linear models to predict Infant health score",
          type = "text", omit.stat = "f",
          se = list(se.fit1, se.fit2, se.fit3))
```

### Testing models for goodness of fit
```{r}
cat("The AIC values for Model 1 - ", AIC(fit1), ", 
    Model 2 - ", AIC(fit2), ", Model 3 - ", AIC(fit3))
```
AIC values indicate that fit3 is a more reliable model in terms of variables than fit1 and fit2.

###Statistical and practical Significance
Regression table we see that both npvis and npvissq are highly statistically significant in all the three models. Mother's age is significant at the 0.05 significance level in models 2 and 3. Practically this means that every prenatal visit(npvis) contributes to an 8.6% increase in the five minute apgar score and npvissq contributes to a 0.1% decrease in the score. The mother's age contributes to a 1.7% increase in the score. The interaction between mother's age and prenatal visit is marginally significant at 0.1 significance level. This contributes to a 0.1% decrease in the score

\newpage
##Causality
6. A discussion of whether your results can be interpretted causally.  In particular, include a discussion of what variables are not included in your analysis and the likely direction of omitted variable bias.  Also include a discussion of which included variables may bias your results by absorbing some of the causal effect of prenatal care.

###Interpretation of causality
From the model we can infer a causal relationship between prenatal visits, mother's age and the five minute apgar score but we have not accounted for unobserved variables like family income, medical history which also influence the score. To establish the causality we need to take into account factors such as income, mother's medical history. If we take into account all the relevant variables we may be able to establish causality but the coefficient will be smaller(since there are other factors contributing to the score as well).

###Omitted variable bias 
cigs(no of cigarettes smoked) - This variable is negatively correlated with both the outcome variable fmaps(-0.01) and independent variable npvis(-0.04) from the correlation matrix.

```{r}
fit4=lm(fmaps ~mage+monpre+npvis+npvissq+mage*npvis+cigs, data=data)    
  
summary(fit2)  
``` 

```{r}
summary(fit4)
```
  
  
In the above comparison of the two models fit2(without cigs) and fit4(with cigs) we see that the coefficient of npvis is positively biased in fit2 by omitting the cigs variable(both the correlations are negative leading to a positive/upward bias)

###Included variable bias
Our model includes both npvis and npvissq. Our hypothesis is that the relationship between fmaps and npvis is linear. Inclusion of npvissq introduces a bias in the coefficients

```{r}
fit6 = lm(fmaps ~mage+npvis+mage*npvis, data=data) 

summary(fit2)  
```

```{r}
summary(fit6)
```
  
  
From the comparison of the models above we see the the coefficient of npvis shows a positive/upward bias in fit2 due to the inclusion of the npvissq variable 
\newpage

##Conclusion and takeaways

Our exploratory analysis and model building show that fit2 is the best model in representing how prenatal care affects new born health conditions. With the limited background information we have about the data,  we have determined that fmaps, or 5 minute APGAR scores, is the best singular variable representation of  infant health. 

We note here that our fit2 has the best adjusted R^2 value. However, our fit3 model has the best (lowest) AIC score. Though a lower AIC score is preferable, it does not necessarily indicate a better model. Thus we go with fit2, our better R^2 model.

Though we cannot deduce a causual relationship, we can infer the explanatory variables of our choice, namely: Mothers age, starting month of prenatal care, number of visits, and our created transformations: number of visits squared and mother's age * number of visits, do have an affect on the baby's health.

Looking at our general model, we can see that number of visits play an important role in the outcome of the baby's health. In terms of practical significance, a mother can look at this model and infer that every additional visit could potentially increase her baby's APGAR score by 8.6%. In addition, many mothers are often worried about having children later in life. Looking at our model alone, ceteris peribus, all other factors constant, it seems that age is not as practically significant as number of visits. Getting pregnant a year or two later, according to the model, might not cause dramatic differences in health, each year resulting in only a 0.1% decrease in the 5 min APGAR score.

Let's also take a second to note an interestingly funny take away from our model as well. One would hypothesize that more alcoholic beverages, termed as the variable "drinks" in our model would result in a negative effect on a baby's health. However, our model shows a slightly positive correlation. So more alcohol means a healthier baby? Ah, not so fast. We did explain that the data for the drinks is heavily skewed to the left (lower amounts of drinks in general).  So that would not be a wise point to take away from the model. Looks like the mothers' whose  drinking data were collected were being responsible after all.

In our analysis, we did exclude certain variables from being part of the model as explanatory variables because they were deemed unfit or problematic as shown in our report., example: drinks. However, if we could expand our analysis, it would be nice to also look at how some excluded variables  such as race and paternal characteristics (father's characterisitics) play a role, as they are more related to genetic influence on the baby's health outcome.
